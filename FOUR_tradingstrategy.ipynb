{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIS IS THE FOURTH NOTEBOOK FOR THE FINAL PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# script for running backtested model\n",
    "# using logistic regression fit to historic\n",
    "# data as predictor\n",
    "#\n",
    "# by Piers Watson\n",
    "# as part of Certificate in Python for Algorithmic Trading\n",
    "#\n",
    "#\n",
    "from tpqoa import tpqoa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tables as tb\n",
    "import tstables as tstb\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, '/root/')\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "# create a logging file for debugging and information purposes\n",
    "import logging    \n",
    "logging.basicConfig(filename=\"jupyter_tradingstrategy.log\",level=logging.DEBUG,\n",
    "                    format=\"%(asctime)s %(name)s.%(funcName)s +%(lineno)s: %(levelname)-8s %(message)s\", \n",
    "                    datefmt ='%d/%m/%y %I:%M:%S %P')\n",
    "\n",
    "class tradingstrategy(tpqoa):\n",
    "    '''class for trading strategy using lagged returns, rsi and macd\n",
    "    indicators. Optimised using logistic regression. Data streamed from\n",
    "    Oanda api.\n",
    "    '''\n",
    "    # special method __init__ which is used to instantiate our python object\n",
    "    def __init__(self, conf_file, instrument):\n",
    "        tpqoa.__init__(self, conf_file)\n",
    "        self.instrument = instrument\n",
    "        self.live_data = pd.DataFrame()\n",
    "        self.position = 0\n",
    "        self.ticks = 0\n",
    "        self.units = 100000\n",
    "        self.rsi_n = 21\n",
    "        self.mom1 = 2\n",
    "        self.mom2 = 5\n",
    "        self.lags = 20\n",
    "        self.model = linear_model.LogisticRegression()\n",
    "        #self.logging.basicConfig(filename=\"jupyter_tradingstrategy.log\",level=logging.DEBUG,\n",
    "        #            format=\"%(asctime)s %(name)s.%(funcName)s +%(lineno)s: %(levelname)-8s %(message)s\", \n",
    "        #            datefmt ='%d/%m/%y %I:%M:%S %P')\n",
    "        \n",
    "    def stream_data(self, stop = None):\n",
    "        ''' starts a real time Oanda data stream\n",
    "        '''\n",
    "        self.ticks = 0\n",
    "        response = self.ctx_stream.pricing.stream(self.account_id, snapshot = True,\n",
    "                                                 instruments = self.instrument)\n",
    "        for msg_type, msg in response.parts():\n",
    "            if msg_type == 'pricing.Price':\n",
    "                self.on_success(msg.time,\n",
    "                              float(msg.bids[0].price),\n",
    "                              float(msg.asks[0].price))\n",
    "                if stop is not None:\n",
    "                    if self.ticks >= stop:\n",
    "                        self.close_out(stop)\n",
    "                        break\n",
    "                            \n",
    "    def on_success(self, time, bid, ask):\n",
    "        '''Method called when new data is received. This updates the on_success\n",
    "        method originally in the tpqoa class inherited by tradingstrategy class\n",
    "        which merely printed and timestamped bid and ask prices'''\n",
    "        self.ticks += 1\n",
    "        if self.ticks % 1 == 0:\n",
    "            print('%3d | %12s | bid %12s | ask %12s'% (self.ticks, time, bid, ask))\n",
    "        # live streaming data append to live_data dataframe    \n",
    "        self.live_data = self.live_data.append(pd.DataFrame({'bid':bid, \n",
    "                                                             'ask': ask},\n",
    "                                                            index = [pd.Timestamp(time)]))\n",
    "        # live streaming data resampled into bars\n",
    "        # resample every 5 seconds for testing...\n",
    "        self.dataresam = self.live_data.resample('5s', label = 'right').last().ffill().iloc[:-1]\n",
    "        # resample every 10 minutes as in model...\n",
    "        #self.dataresam = self.live_data.resample('10T', label = 'right').last().ffill().iloc[:-1]\n",
    "        # having resmpled data calculate mid and therefore period return %\n",
    "        self.dataresam['mid'] = self.dataresam.mean(axis=1)\n",
    "        self.dataresam['returns'] = np.log(self.dataresam['mid'] / self.dataresam['mid'].shift(1))\n",
    "        # if the length of the dataresam is large enough start calculating features\n",
    "        if len(self.dataresam) > 22: \n",
    "            # call relative strength_method to calculate RSI Index\n",
    "            self.dataresam['RSI'] = self.relative_strength(self.dataresam['mid'], self.rsi_n)\n",
    "            # call macd method to calculate macd index\n",
    "            self.dataresam['MACD'] = self.macd(self.dataresam['mid'])\n",
    "            # call prepare_features method to calculate lagged, returns, RSI, macd and also mom\n",
    "            self.dataresam = self.prepare_features(self.dataresam, self.lags)\n",
    "            # call load_model method to use saved model\n",
    "            self.load_model()\n",
    "    \n",
    "    def relative_strength(self, data, rsi_n):\n",
    "        '''Creates RSI feature -\n",
    "        initial RSI value created here\n",
    "        '''\n",
    "        abchange = (data - data.shift(1)) # calculate absolute daily change\n",
    "        rsperiod = abchange[:rsi_n + 1]\n",
    "        upday = rsperiod[rsperiod >= 0].sum() / rsi_n # in the RSI period what is the up day change\n",
    "        dnday = -rsperiod[rsperiod < 0].sum() / rsi_n # in the RSI period what is the down day change\n",
    "        rs = upday / dnday # up day change/down day change ratio\n",
    "        rsi = np.zeros_like(data)\n",
    "        rsi[:rsi_n] = 100. - (100. / ( 1. + rs)) # formula for RSI Index calculation\n",
    "        \n",
    "        '''calculates subsequent change in RSI values'''\n",
    "        for i in range(rsi_n, len(data)):\n",
    "            abchg = abchange[i - 1]\n",
    "            if abchg > 0:\n",
    "                upval = abchg\n",
    "                dnval = 0\n",
    "            else:\n",
    "                upval = 0\n",
    "                dnval = abs(abchg)\n",
    "            \n",
    "            # iterate through each daily change proportionally adding it\n",
    "            # to the respective RSI period change\n",
    "            upday = (upday * (rsi_n - 1) + upval) / rsi_n\n",
    "            dnday = (dnday * (rsi_n - 1) + dnval) / rsi_n\n",
    "            \n",
    "            rs = upday / dnday # up day change/down day change ratio\n",
    "            rsi[i] = 100. - (100. / ( 1. + rs)) # formula for RSI Index calculation\n",
    "        rsi = pd.DataFrame(rsi)\n",
    "        rsi.index = data.index\n",
    "        rsi.columns = ['RSI']\n",
    "        return rsi # Return the RSI Index value calculated\n",
    "    \n",
    "    def macd(self, data, slow = 26, fast = 12, signal = 9):\n",
    "        # calculate respective fast and slow exponential moving averages\n",
    "        ema_fast = data.ewm(span = fast).mean()\n",
    "        ema_slow = data.ewm(span = slow).mean()\n",
    "        # MACD line is slow m.a. minus fast m.a.\n",
    "        macd_line = ema_slow - ema_fast\n",
    "        # signal line is 9 day ema of macd line\n",
    "        sig_line = macd_line.ewm(span = signal).mean()\n",
    "        # macd histogram is the macd line minus the signal line\n",
    "        macd_hist = macd_line - sig_line\n",
    "        macd_hist = pd.DataFrame(macd_hist)\n",
    "        macd_hist.columns = ['MACD']\n",
    "        return macd_hist\n",
    "            \n",
    "    def prepare_features(self, df, lagz):\n",
    "        '''creates lagged and momentum features'''\n",
    "        self.cols = []\n",
    "        \n",
    "        #self.features = ['RSI','MACD','Returns']\n",
    "        # add lagged RSI and MACD data, backtest suggests 1\n",
    "        # lagged return only\n",
    "        for feat1 in ['RSI','MACD']:\n",
    "            lag1 = 1\n",
    "            col = '%s_lag_%d' % (feat1, lag1)\n",
    "            df[col] = df[feat1].shift(lag1)\n",
    "            self.cols.append(col)\n",
    "            \n",
    "        # add lagged return data, backtest suggests 20\n",
    "        # lagged returns        \n",
    "        for lag in range(1,lagz + 1):\n",
    "            col = 'Returns_lag_%d' % lag\n",
    "            df[col] = df['returns'].shift(lag)\n",
    "            self.cols.append(col)\n",
    "            \n",
    "        # add short term momentum signal\n",
    "        df['MOM1'] = np.where(df['returns'].rolling(self.mom1).mean() > 0, 1, 0)\n",
    "        df['MOM1'] = df['MOM1'].shift(1)\n",
    "        self.cols.append('MOM1')\n",
    "        # add long term momentum signal\n",
    "        df['MOM2'] = np.where(df['returns'].rolling(self.mom2).mean() > 0, 1, 0)\n",
    "        df['MOM2'] = df['MOM2'].shift(1)\n",
    "        self.cols.append('MOM2')\n",
    "        df.dropna(inplace = True)\n",
    "        return df\n",
    "    \n",
    "    def load_model(self):\n",
    "        '''model determined by backtesting has been saved in pickle\n",
    "        we call the model here applying features created by live data\n",
    "        '''\n",
    "        LinMod = pickle.load(open('final_model.sav','rb'))\n",
    "        pred = LinMod.predict(self.dataresam[self.cols])\n",
    "        self.dataresam['prediction'] = pred\n",
    "        # call method execute_order to submit trades to market\n",
    "        self.execute_order()\n",
    "        \n",
    "    def execute_order(self):\n",
    "        # Entering long\n",
    "        if self.dataresam['prediction'].iloc[-2] > 0 and self.position == 0:\n",
    "            #print('going long')\n",
    "            print('going long | %s | units %4d | ask %0.5f' % \n",
    "                         (self.instrument, self.units, self.dataresam.iloc[-1]['ask']))\n",
    "            self.position = 1\n",
    "            self.create_order(self.instrument, self.units)\n",
    "            logging.info('going long | %s | units %4d | ask %0.5f' % \n",
    "                         (self.instrument, self.units, self.dataresam.iloc[-1]['ask']))\n",
    "            \n",
    "        elif self.dataresam['prediction'].iloc[-2] > 0 and self.position == -1:\n",
    "            #print('covering short and going long')\n",
    "            print('covering short and going long | %s | units %4d | ask %0.5f' %\n",
    "                         (self.instrument, self.units, self.dataresam.iloc[-1]['ask']))\n",
    "            self.position = 1\n",
    "            self.create_order(self.instrument, 2 * self.units)\n",
    "            logging.info('covering short and going long | %s | units %4d | ask %0.5f' %\n",
    "                         (self.instrument, self.units, self.dataresam.iloc[-1]['ask']))\n",
    "            \n",
    "        # Entering short\n",
    "        elif self.dataresam['prediction'].iloc[-2] < 0 and self.position == 0:\n",
    "            #print('going short')\n",
    "            print('going short | %s | units %4d | bid %0.5f' % \n",
    "                         (self.instrument, self.units, self.dataresam.iloc[-1]['bid']))\n",
    "            self.position = -1\n",
    "            self.create_order(self.instrument, units = -self.units)\n",
    "            logging.info('going short | %s | units %4d | bid %0.5f' % \n",
    "                         (self.instrument, self.units, self.dataresam.iloc[-1]['bid']))\n",
    "            \n",
    "        elif self.dataresam['prediction'].iloc[-2] < 0 and self.position == 1:\n",
    "            #print('covering long and going short')\n",
    "            print('covering long and going short | %s | units %4d | bid %0.5f' %\n",
    "                         (self.instrument, self.units, self.dataresam.iloc[-1]['bid']))\n",
    "            self.position = -1\n",
    "            self.create_order(self.instrument, units = -2 * self.units)\n",
    "            logging.info('covering long and going short | %s | units %4d | bid %0.5f' %\n",
    "                         (self.instrument, self.units, self.dataresam.iloc[-1]['bid']))      \n",
    "                \n",
    "    def close_out(self, stop):\n",
    "        if self.ticks >= stop:\n",
    "            logging.info('stop reached')\n",
    "        \n",
    "            # stop reached close out long position\n",
    "            if self.position == 1:\n",
    "                self.create_order(self.instrument, \n",
    "                                  units = -self.units)\n",
    "                logging.info('stop reached - closing long, no open positions| %s | units %4d | bid %0.5f' \n",
    "                             % (self.instrument, self.units, self.dataresam.iloc[-1]['bid']))\n",
    "                print(15 * '-')\n",
    "                print('stop reached - closing long, no open positions') \n",
    "                print(15 * '-')\n",
    "                self.position = 0\n",
    "                \n",
    "            # stop reached close out short position\n",
    "            elif self.position == -1:\n",
    "                self.create_order(self.instrument, units = self.units)\n",
    "                logging.info('stop reached - closing short, no open position| %s | units %4d | ask %0.5f' \n",
    "                             % (self.instrument, self.units, self.dataresam.iloc[-1]['ask']))\n",
    "                print(15 * '-')\n",
    "                print('stop reached - closing short, no open positions')\n",
    "                print(15 * '-')\n",
    "                self.position = 0\n",
    "            # id - needs to be set to the lastest order id in your Oanda transactions\n",
    "            # report\n",
    "            self.response = self.ctx.transaction.since(self.account_id, id = 3058)\n",
    "            self.transactions = self.response.get('transactions')\n",
    "            for trans in self.transactions:\n",
    "                trans = trans.dict()\n",
    "                if trans['type']  == 'ORDER_FILL':\n",
    "                    templ = '%s | id %8s | %6s | %12s | price %12s | p&l %8s'\n",
    "                    print(templ % (trans['time'], trans['orderID'], trans['instrument'],trans['units'], \n",
    "                                   trans['price'], trans['pl']))\n",
    "                    #               trans['fullPrice']['bids'][0]['price'],\n",
    "                    logging.info(templ % (trans['time'], trans['orderID'], trans['instrument'],trans['units'],\n",
    "                                          trans['price'], trans['pl']))            \n",
    "\n",
    "                \n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = tradingstrategy('/root/pyalgo.cfg','AUD_USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 | 2018-05-30T06:17:49.761774102Z | bid        0.751 | ask      0.75113\n",
      "  2 | 2018-05-30T06:17:54.204654357Z | bid      0.75094 | ask      0.75108\n",
      "  3 | 2018-05-30T06:18:12.893684421Z | bid        0.751 | ask      0.75113\n",
      "  4 | 2018-05-30T06:18:19.736117155Z | bid      0.75105 | ask       0.7512\n",
      "  5 | 2018-05-30T06:18:20.054799805Z | bid      0.75103 | ask      0.75116\n",
      "  6 | 2018-05-30T06:18:22.046462201Z | bid      0.75109 | ask      0.75123\n",
      "  7 | 2018-05-30T06:18:22.831189401Z | bid      0.75108 | ask      0.75123\n",
      "  8 | 2018-05-30T06:18:23.000216613Z | bid      0.75101 | ask      0.75116\n",
      "  9 | 2018-05-30T06:18:25.884552493Z | bid      0.75108 | ask      0.75121\n",
      " 10 | 2018-05-30T06:18:32.907592391Z | bid      0.75114 | ask      0.75127\n",
      " 11 | 2018-05-30T06:18:36.377160511Z | bid      0.75119 | ask      0.75133\n",
      " 12 | 2018-05-30T06:18:38.765050940Z | bid      0.75119 | ask      0.75132\n",
      " 13 | 2018-05-30T06:18:42.447471749Z | bid      0.75118 | ask      0.75134\n",
      " 14 | 2018-05-30T06:18:42.476169453Z | bid      0.75115 | ask       0.7513\n",
      " 15 | 2018-05-30T06:18:46.168684621Z | bid      0.75121 | ask      0.75135\n",
      " 16 | 2018-05-30T06:18:49.896049991Z | bid      0.75119 | ask      0.75135\n",
      " 17 | 2018-05-30T06:18:50.233695098Z | bid      0.75118 | ask      0.75132\n",
      " 18 | 2018-05-30T06:18:51.018607010Z | bid      0.75117 | ask      0.75132\n",
      " 19 | 2018-05-30T06:18:52.713639523Z | bid      0.75111 | ask      0.75126\n",
      " 20 | 2018-05-30T06:18:59.263430479Z | bid      0.75108 | ask      0.75122\n",
      " 21 | 2018-05-30T06:19:03.730578913Z | bid      0.75114 | ask      0.75129\n",
      " 22 | 2018-05-30T06:19:03.760553294Z | bid      0.75114 | ask       0.7513\n",
      " 23 | 2018-05-30T06:19:08.915451978Z | bid      0.75113 | ask      0.75128\n",
      " 24 | 2018-05-30T06:19:11.172058786Z | bid      0.75119 | ask      0.75134\n",
      " 25 | 2018-05-30T06:19:11.356504513Z | bid      0.75118 | ask      0.75134\n",
      "2018-05-30T03:45:08.043118876Z | id     3058 | AUD_USD |    -100000.0 | price      0.74951 | p&l -24.0157\n"
     ]
    }
   ],
   "source": [
    "rt.stream_data(stop = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
